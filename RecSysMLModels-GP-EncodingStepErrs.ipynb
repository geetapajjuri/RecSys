{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d70e13",
   "metadata": {},
   "source": [
    "#### We'll will compare the following models for their accuracy and precision. \n",
    "\n",
    "1. Collaborative Filtering (Item-Based)\n",
    "2. Markov Chains\n",
    "3. Random Forest\n",
    "4. Gradient Boosting Machine\n",
    "5. Recurrent Neural Networks (RNN) - LSTM\n",
    "\n",
    "To get us started we will set up the preprocessing and helper functions that will be used by all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd41089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591e7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24740cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad80e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dc5b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/20vjqs8n48144z539zr12fcc0000gr/T/ipykernel_83625/1418862632.py:5: DtypeWarning: Columns (1,2,4,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv('train_set_1.csv')\n",
      "/var/folders/m7/20vjqs8n48144z539zr12fcc0000gr/T/ipykernel_83625/1418862632.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  train_data['checkin'] = pd.to_datetime(train_data['checkin'])\n",
      "/var/folders/m7/20vjqs8n48144z539zr12fcc0000gr/T/ipykernel_83625/1418862632.py:10: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  train_data['checkout'] = pd.to_datetime(train_data['checkout'])\n",
      "/var/folders/m7/20vjqs8n48144z539zr12fcc0000gr/T/ipykernel_83625/1418862632.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  test_data['checkin'] = pd.to_datetime(test_data['checkin'])\n",
      "/var/folders/m7/20vjqs8n48144z539zr12fcc0000gr/T/ipykernel_83625/1418862632.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  test_data['checkout'] = pd.to_datetime(test_data['checkout'])\n"
     ]
    }
   ],
   "source": [
    "# Initialize tqdm for progress tracking\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train_set_1.csv')\n",
    "test_data = pd.read_csv('test_set_1.csv')\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "train_data['checkin'] = pd.to_datetime(train_data['checkin'])\n",
    "train_data['checkout'] = pd.to_datetime(train_data['checkout'])\n",
    "test_data['checkin'] = pd.to_datetime(test_data['checkin'])\n",
    "test_data['checkout'] = pd.to_datetime(test_data['checkout'])\n",
    "\n",
    "# Sort the data by user trip ID and check-in date to maintain the chronological order\n",
    "train_data.sort_values(by=['utrip_id', 'checkin'], inplace=True)\n",
    "test_data.sort_values(by=['utrip_id', 'checkin'], inplace=True)\n",
    "\n",
    "# Create a city_country column\n",
    "train_data['city_country'] = train_data['city_id'].astype(str) + '_' + train_data['hotel_country'].astype(str)\n",
    "test_data['city_country'] = test_data['city_id'].astype(str) + '_' + test_data['hotel_country'].astype(str)\n",
    "\n",
    "# Group by utrip_id to create sequences\n",
    "train_sequences = train_data.groupby('utrip_id')['city_country'].apply(list).tolist()\n",
    "test_sequences = test_data.groupby('utrip_id')['city_country'].apply(list).tolist()\n",
    "\n",
    "# Encode city_country strings as integers\n",
    "encoder = LabelEncoder()\n",
    "all_cities_countries = [city_country for seq in train_sequences + test_sequences for city_country in seq]\n",
    "encoder.fit(all_cities_countries)\n",
    "encoded_train_sequences = [encoder.transform(seq) for seq in train_sequences]\n",
    "encoded_test_sequences = [encoder.transform(seq) for seq in test_sequences]\n",
    "\n",
    "# Prepare data for training models\n",
    "def prepare_data(sequences):\n",
    "    X, y = [], []\n",
    "    for seq in sequences:\n",
    "        for i in range(1, len(seq)):\n",
    "            X.append(seq[:i])\n",
    "            y.append(seq[i])\n",
    "    X = pad_sequences(X, padding='pre')\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = prepare_data(encoded_train_sequences)\n",
    "X_test, y_test = prepare_data(encoded_test_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57608e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of X_train:\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0 6377]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0 6377  630]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 6377  630 5492]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0 3088]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0 3088 4569]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 3088 4569 1237]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 3088 4569 1237 1913]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0 5927]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0 5927 4908]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 5927 4908 6521]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 5927 4908 6521 2377]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0 5927 4908 6521 2377 2823]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  5927 4908 6521 2377 2823 5219]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0 4995]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0 4995  524]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 4995  524 2728]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0  152]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0  152  267]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0  152  267 6224]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0  152  267 6224 6227]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0  785]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0  785 2302]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0  785 2302 3287]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0  785 2302 3287 4908]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0  785 2302 3287 4908 1203]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   785 2302 3287 4908 1203 5720]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0  785\n",
      "  2302 3287 4908 1203 5720 6232]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0  785 2302\n",
      "  3287 4908 1203 5720 6232 6232]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  785 2302 3287\n",
      "  4908 1203 5720 6232 6232 5523]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0 5655]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0 5655 3507]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 5655 3507 1126]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 5655 3507 1126 2572]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0 4058]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0 4058 1874]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 4058 1874  850]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 4058 1874  850 5077]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0  858]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0  858 5156]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0  858 5156 1329]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0  858 5156 1329 1912]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0  858 5156 1329 1912 3977]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   858 5156 1329 1912 3977 5195]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0 5422]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0 5422 1003]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 5422 1003 6486]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 5422 1003 6486 4557]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0 1400]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0 1400  151]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 1400  151 2230]]\n",
      "\n",
      "First 5 elements of y_train:\n",
      "[ 630 5492 2250 4569 1237 1913 3088 4908 6521 2377 2823 5219 2814  524\n",
      " 2728 2814  267 6224 6227 6012 2302 3287 4908 1203 5720 6232 6232 5523\n",
      " 3510 3507 1126 2572 2469 1874  850 5077 2786 5156 1329 1912 3977 5195\n",
      " 5166 1003 6486 4557 5097  151 2230 2834]\n",
      "\n",
      "First 5 rows of X_train (as DataFrame):\n",
      "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16    17  \\\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0     0   \n",
      "1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0     0   \n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  6377   \n",
      "3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0     0   \n",
      "4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0     0   \n",
      "\n",
      "     18    19  \n",
      "0     0  6377  \n",
      "1  6377   630  \n",
      "2   630  5492  \n",
      "3     0  3088  \n",
      "4  3088  4569  \n",
      "\n",
      "First 5 elements of y_train (as DataFrame):\n",
      "0     630\n",
      "1    5492\n",
      "2    2250\n",
      "3    4569\n",
      "4    1237\n",
      "Name: Target, dtype: int64\n",
      "\n",
      "First 5 rows of X_train (as DataFrame):\n",
      "   0   1   2   3   4   5   6   7   8   9   ...  11  12  13  14  15  16  17  \\\n",
      "0   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "1   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "2   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "3   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "4   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "\n",
      "     18    19    20  \n",
      "0     0     0  5047  \n",
      "1     0  5047  3452  \n",
      "2  5047  3452  5198  \n",
      "3     0     0  4338  \n",
      "4     0  4338  2095  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "First 5 elements of y_train (as DataFrame):\n",
      "0    3452\n",
      "1    5198\n",
      "2       0\n",
      "3    2095\n",
      "4    3750\n",
      "Name: Target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 rows of X_train and y_train\n",
    "print(\"First 5 rows of X_train:\")\n",
    "print(X_train[:50])\n",
    "\n",
    "print(\"\\nFirst 5 elements of y_train:\")\n",
    "print(y_train[:50])\n",
    "\n",
    "# Convert to pandas DataFrame for better readability\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "y_train_df = pd.Series(y_train, name='Target')\n",
    "\n",
    "# Convert to pandas DataFrame for better readability\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "y_test_df = pd.Series(y_test, name='Target')\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"\\nFirst 5 rows of X_train (as DataFrame):\")\n",
    "print(X_train_df.head())\n",
    "\n",
    "print(\"\\nFirst 5 elements of y_train (as DataFrame):\")\n",
    "print(y_train_df.head())\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"\\nFirst 5 rows of X_train (as DataFrame):\")\n",
    "print(X_test_df.head())\n",
    "\n",
    "print(\"\\nFirst 5 elements of y_train (as DataFrame):\")\n",
    "print(y_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b03823d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unique_city_country \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(city_country \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences \u001b[38;5;28;01mfor\u001b[39;00m city_country \u001b[38;5;129;01min\u001b[39;00m seq)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique city_country values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, unique_city_country)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequences' is not defined"
     ]
    }
   ],
   "source": [
    "unique_city_country = set(city_country for seq in sequences for city_country in seq)\n",
    "print(\"Unique city_country values:\", unique_city_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd1082",
   "metadata": {},
   "source": [
    "### Collaborative Filtering (Item-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00dddb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collab Filtering Complete\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Create item-based collaborative filtering model\n",
    "#item_sim_matrix = cosine_similarity(np.identity(len(encoder.classes_)))\n",
    "\n",
    "# Initialize tqdm for progress tracking\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "classes = encoder.classes_\n",
    "\n",
    "# Create one-hot encoded matrix\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(classes.reshape(-1, 1))\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "item_sim_matrix = cosine_similarity(onehot_encoded)\n",
    "\n",
    "def collaborative_filtering_predict(current_place):\n",
    "    if current_place in encoder.classes_:\n",
    "        current_idx = encoder.transform([current_place])[0]\n",
    "        most_similar_idx = np.argmax(item_sim_matrix[current_idx])\n",
    "        return encoder.inverse_transform([most_similar_idx])[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "collab_preds = [collaborative_filtering_predict(encoder.inverse_transform([seq[-1]])[0]) for seq in encoded_test_sequences]\n",
    "print(\"Collab Filtering Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83670233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee2d20d0",
   "metadata": {},
   "source": [
    "### Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c7026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov Complete\n"
     ]
    }
   ],
   "source": [
    "# Create transition pairs from the city_country chains\n",
    "transitions = []\n",
    "\n",
    "for chain in train_sequences:\n",
    "    for i in range(len(chain) - 1):\n",
    "        transitions.append((chain[i], chain[i + 1]))\n",
    "\n",
    "# Create a DataFrame for transitions\n",
    "transitions_df = pd.DataFrame(transitions, columns=['current_place', 'next_place'])\n",
    "\n",
    "# Calculate transition probabilities\n",
    "transition_counts = transitions_df.groupby('current_place')['next_place'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "\n",
    "# Function to predict the next place based on the current place\n",
    "def markov_chain_predict(current_place):\n",
    "    if current_place in transition_counts.index:\n",
    "        return transition_counts.loc[current_place].idxmax()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "markov_preds = [markov_chain_predict(encoder.inverse_transform([seq[-1]])[0]) for seq in encoded_test_sequences]\n",
    "print(\"Markov Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aae2ad",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8db1ad16",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 21 features, but RandomForestClassifier is expecting 20 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Predict the next city_country\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m rf_preds \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      9\u001b[0m rf_preds \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39minverse_transform(rf_preds)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest Complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:820\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    800\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:862\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    860\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X)\n\u001b[1;32m    864\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    865\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:602\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 602\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 21 features, but RandomForestClassifier is expecting 20 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train the random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the next city_country\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "rf_preds = encoder.inverse_transform(rf_preds)\n",
    "\n",
    "print(\"Random Forest Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1014f0",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d92c0779",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 21 features, but GradientBoostingClassifier is expecting 20 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m gbm_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Predict the next city_country\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m gbm_preds \u001b[38;5;241m=\u001b[39m gbm_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      9\u001b[0m gbm_preds \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39minverse_transform(gbm_preds)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGBM Complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:1308\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict class for X.\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \n\u001b[1;32m   1296\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1308\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n\u001b[1;32m   1309\u001b[0m     encoded_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss\u001b[38;5;241m.\u001b[39m_raw_prediction_to_decision(raw_predictions)\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(encoded_labels, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:1261\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;124;03m        array of shape (n_samples,).\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1261\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1262\u001b[0m         X, dtype\u001b[38;5;241m=\u001b[39mDTYPE, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m     )\n\u001b[1;32m   1264\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_predict(X)\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 21 features, but GradientBoostingClassifier is expecting 20 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create and train the gradient boosting model\n",
    "gbm_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the next city_country\n",
    "gbm_preds = gbm_model.predict(X_test)\n",
    "gbm_preds = encoder.inverse_transform(gbm_preds)\n",
    "\n",
    "print(\"GBM Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716cd7a1",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNN) - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cda796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=len(encoder.classes_), output_dim=50, input_length=X_train.shape[1]))\n",
    "lstm_model.add(LSTM(100, return_sequences=False))\n",
    "lstm_model.add(Dense(len(encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Predict the next city_country\n",
    "lstm_preds = lstm_model.predict(X_test)\n",
    "lstm_preds = np.argmax(lstm_preds, axis=1)\n",
    "lstm_preds = encoder.inverse_transform(lstm_preds)\n",
    "\n",
    "print(\"LSTM Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0427a4",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84261b0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: '3452'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:164\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:158\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '3452'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, precision\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Evaluate all models\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#collab_accuracy, collab_precision = evaluate_model(y_test, collab_preds)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m markov_accuracy, markov_precision \u001b[38;5;241m=\u001b[39m evaluate_model(y_test, markov_preds)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#rf_accuracy, rf_precision = evaluate_model(y_test, rf_preds)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#gbm_accuracy, gbm_precision = evaluate_model(y_test, gbm_preds)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#lstm_accuracy, lstm_precision = evaluate_model(y_test, lstm_preds)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#print(f\"Collaborative Filtering - Accuracy: {collab_accuracy:.2f}, Precision: {collab_precision:.2f}\")\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarkov Chains - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmarkov_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmarkov_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(y_true, y_pred):\n\u001b[0;32m----> 3\u001b[0m     y_true_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(y_true)\n\u001b[1;32m      4\u001b[0m     y_pred_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(y_pred)\n\u001b[1;32m      5\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_true_encoded, y_pred_encoded)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:139\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:226\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: '3452'"
     ]
    }
   ],
   "source": [
    "# Helper function to evaluate models\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    y_true_encoded = encoder.transform(y_true)\n",
    "    y_pred_encoded = encoder.transform(y_pred)\n",
    "    accuracy = accuracy_score(y_true_encoded, y_pred_encoded)\n",
    "    precision = precision_score(y_true_encoded, y_pred_encoded, average='weighted')\n",
    "    return accuracy, precision\n",
    "\n",
    "# Evaluate all models\n",
    "#collab_accuracy, collab_precision = evaluate_model(y_test, collab_preds)\n",
    "markov_accuracy, markov_precision = evaluate_model(y_test, markov_preds)\n",
    "#rf_accuracy, rf_precision = evaluate_model(y_test, rf_preds)\n",
    "#gbm_accuracy, gbm_precision = evaluate_model(y_test, gbm_preds)\n",
    "#lstm_accuracy, lstm_precision = evaluate_model(y_test, lstm_preds)\n",
    "\n",
    "# Print the results\n",
    "#print(f\"Collaborative Filtering - Accuracy: {collab_accuracy:.2f}, Precision: {collab_precision:.2f}\")\n",
    "print(f\"Markov Chains - Accuracy: {markov_accuracy:.2f}, Precision: {markov_precision:.2f}\")\n",
    "#print(f\"Random Forest - Accuracy: {rf_accuracy:.2f}, Precision: {rf_precision:.2f}\")\n",
    "#print(f\"Gradient Boosting - Accuracy: {gbm_accuracy:.2f}, Precision: {gbm_precision:.2f}\")\n",
    "#print(f\"LSTM - Accuracy: {lstm_accuracy:.2f}, Precision: {lstm_precision:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert collab_preds to DataFrame\n",
    "collab_preds_df = pd.DataFrame(collab_preds, columns=['predicted_next_city_country'])\n",
    "\n",
    "# Optionally, add a column for the original test sequences for reference\n",
    "# Assuming you want to add the last element from each sequence as the current city\n",
    "current_city = [encoder.inverse_transform([seq[-1]])[0] for seq in encoded_test_sequences]\n",
    "collab_preds_df['current_city_country'] = current_city\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = 'collab_predictions.csv'\n",
    "collab_preds_df.to_csv(output_file, index=False)\n",
    "print(f'Predictions written to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cea716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert markov_preds to DataFrame\n",
    "markov_preds_df = pd.DataFrame(markov_preds, columns=['predicted_next_city_country'])\n",
    "\n",
    "# Optionally, add a column for the original test sequences for reference\n",
    "# Assuming you want to add the last element from each sequence as the current city\n",
    "current_city = [encoder.inverse_transform([seq[-1]])[0] for seq in encoded_test_sequences]\n",
    "markov_preds_df['current_city_country'] = current_city\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = 'markov_predictions.csv'\n",
    "markov_preds_df.to_csv(output_file, index=False)\n",
    "print(f'Predictions written to {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52762667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert gbm_preds to DataFrame\n",
    "gbm_preds_df = pd.DataFrame(gbm_preds, columns=['predicted_next_city_country'])\n",
    "\n",
    "# Optionally, add a column for the original test sequences for reference\n",
    "# Assuming you want to add the last element from each sequence as the current city\n",
    "current_city = [encoder.inverse_transform([seq[-1]])[0] for seq in encoded_test_sequences]\n",
    "gbm_preds_df['current_city_country'] = current_city\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = 'gbm_predictions.csv'\n",
    "gbm_preds_df.to_csv(output_file, index=False)\n",
    "print(f'Predictions written to {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lstm_preds to DataFrame\n",
    "lstm_preds_df = pd.DataFrame(lstm_preds, columns=['predicted_next_city_country'])\n",
    "\n",
    "# Optionally, add a column for the original test sequences for reference\n",
    "# Assuming you want to add the last element from each sequence as the current city\n",
    "current_city = [encoder.inverse_transform([seq[-1]])[0] for seq in encoded_test_sequences]\n",
    "lstm_preds_df['current_city_country'] = current_city\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = 'lstm_predictions.csv'\n",
    "lstm_preds_df.to_csv(output_file, index=False)\n",
    "print(f'Predictions written to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916b0d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
